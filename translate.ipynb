{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: no duplicate entries in dictionary_az.\n",
      "Then, man will be able to reach freedom as the condor.\n",
      "--------------------------------------------\n",
      "then             | hinaspa, (then,1.00) ,\n",
      "man              | runa (man,1.00) \n",
      "will             | n (will,1.00) n\n",
      "be               | ñulaysaqn (be,1.00) saqn\n",
      "able             | qutisaqn (able,1.00) saqn\n",
      "to               |  (to,1.00) \n",
      "reach            | aypay (reach,1.00) \n",
      "freedom          | qispiypaq (freedom,1.00) paq\n",
      "as               | hina (as,1.00) \n",
      "the              |  (the,1.00) \n",
      "condor           | qunturhina. (condor,1.00) hina.\n",
      "Words not in dictionary: []\n",
      "Unable to match these words: []\n",
      "Then, man will be able to reach freedom as the condor.\n",
      "Hinaspa, runa n ñulaysaqn qutisaqn aypay qispiypaq hina qunturhina.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "default = \"The condor, cougar and serpent represent the three parts of our world. Each level contains a special symbol and specific purpose. It is our duty to understand, learn and protect our three symbols. For man to reach his fullest capacity, he must go through all three levels. Man needs to shed like a snake and become steady as a cougar. Then, man will be able to reach freedom as the condor.\"\n",
    "dictionary = {}\n",
    "r = round\n",
    "l = len\n",
    "sim_thresh = 0.5\n",
    "debug = True\n",
    "\n",
    "def jaccard(word, other):\n",
    "    i = len(set(word).intersection(set(other)))\n",
    "    u = len(set(word).union(set(other)))\n",
    "    return i/u\n",
    "\n",
    "def read_dict(filename, dictionary):\n",
    "    n=len(dictionary)\n",
    "    with open(filename+\".csv\") as fp:\n",
    "        for line in fp:\n",
    "            line = line.lower().strip().split(',')\n",
    "            if line[0] not in dictionary:\n",
    "                dictionary [line[0].strip()] = [line[2],line[1]]\n",
    "                n+=1\n",
    "        if len(dictionary) == n: print(f\"Message: no duplicate entries in {filename}.\")\n",
    "        else: print(f\"Warning: duplicate entries detected in {filename}: {n -len(dictionary)}\")\n",
    "    return dictionary\n",
    "\n",
    "def is_noun(word):\n",
    "    try:\n",
    "        noun = dictionary[word][1] == \"noun\"\n",
    "    except:\n",
    "        pl = False\n",
    "        if word[-1] == \"s\":\n",
    "            word = word[:-1]\n",
    "            pl = True\n",
    "            \n",
    "        try:\n",
    "            noun = dictionary[word][1] == \"noun\"\n",
    "        except:\n",
    "            if pl:\n",
    "                word += \"s\"\n",
    "            word_approx = re.search(\"\\(\\w+,\",find_similar(word, dictionary)).group()[1:-1]\n",
    "            noun = dictionary[word_approx][1] == \"noun\"\n",
    "    return noun\n",
    "\n",
    "def is_verb(word):\n",
    "    try:\n",
    "        verb = dictionary[word][1] == \"verb\"\n",
    "    except:\n",
    "        p2 = False\n",
    "        if word[-1] == \"s\":\n",
    "            word = word[:-1]\n",
    "            p2 = True\n",
    "        try:\n",
    "            verb = dictionary[word][1] == \"verb\"\n",
    "        except:\n",
    "            if p2:\n",
    "                word += \"s\"\n",
    "            word_approx = re.search(\"\\(\\w+,\",find_similar(word, dictionary)).group()[1:-1]\n",
    "            verb = dictionary[word_approx][1] == \"verb\"\n",
    "    return verb\n",
    "\n",
    "def is_plural(word):\n",
    "    return is_noun(word) and word[-1] == \"s\"\n",
    "\n",
    "def object_casing(word_obj):\n",
    "    if word_obj[\"noun\"]:\n",
    "        suffixes = {\"with\": \"wan\", #instrumental\n",
    "                    \"without\": \"naq\", #abessive\n",
    "                    \"for\": \"paq\", #dative\n",
    "                    \"to\": \"paq\", #dative\n",
    "                    \"of\": \"pa\", #genitive\n",
    "                    \"than\": \"hina\", #comparative\n",
    "                    \"as\": \"hina\", #comparative\n",
    "                    \"like\": \"hina\" #comparative\n",
    "        }\n",
    "        for preposition in word_obj[\"context\"][::-1]:\n",
    "            if preposition in suffixes:\n",
    "                word_obj[\"tail\"] = suffixes[preposition] + word_obj[\"tail\"]\n",
    "                return word_obj\n",
    "        if word_obj[\"object\"]: word_obj[\"tail\"] = \"ta\" + word_obj[\"tail\"] #accusative\n",
    "    return word_obj\n",
    "\n",
    "def verb_conjugation(word_obj):\n",
    "    if word_obj[\"verb\"] and \"to\" not in word_obj[\"context\"]:\n",
    "        person = \"\"\n",
    "        plural = \"\"\n",
    "        suffixes = [\n",
    "            {\n",
    "                \"1\":\"ni\",\n",
    "                \"2\":\"nqi\",\n",
    "                \"3\":\"n\",\n",
    "            },\n",
    "            {\n",
    "                \"1\":\"cqu\",\n",
    "                \"2\":\"nqu\",\n",
    "                \"3\":\"nqu\"\n",
    "            }\n",
    "        ]\n",
    "        tenses = {\n",
    "            \"will\":\"saq\",\n",
    "            \"was\":\"rqa\"\n",
    "        }\n",
    "        noun_cnt = 0\n",
    "        for item in word_obj[\"context\"]:\n",
    "            noun_cnt += is_noun(item)\n",
    "        if noun_cnt > 1 or \"we\" in word_obj[\"context\"] or \"they\" in word_obj[\"context\"]:\n",
    "            word_obj[\"plural\"] = True\n",
    "            if \"he\" in word_obj[\"context\"] or \"she\" in word_obj[\"context\"]:\n",
    "                word_obj[\"plural\"] = False\n",
    "        \n",
    "        if \"i\" in word_obj[\"context\"] or \"we\" in word_obj[\"context\"]: person = \"1\"\n",
    "        elif \"you\" in word_obj[\"context\"]: person = \"2\"\n",
    "        else: person = \"3\"\n",
    "        word_obj[\"tail\"] = suffixes[word_obj[\"plural\"]][person] + word_obj[\"tail\"]\n",
    "        for indic in word_obj[\"context\"][::-1]:\n",
    "            if indic in tenses:\n",
    "                word_obj[\"tail\"] = tenses[indic] + word_obj[\"tail\"]\n",
    "    return word_obj\n",
    "\n",
    "def find_similar(word, dictionary):\n",
    "    max_sim = 0\n",
    "    for other in dictionary:\n",
    "        new_sim = jaccard(word, other)\n",
    "        if new_sim > max_sim:\n",
    "            winner, max_sim = other, new_sim\n",
    "    if max_sim > sim_thresh: qic_word = dictionary[winner][0] + f\" ({winner},{r(max_sim,2)})\"\n",
    "    else: qic_word = word + f\" ({winner},{r(max_sim,2)})\"\n",
    "    return qic_word\n",
    "\n",
    "def preprocess(word, word_obj):\n",
    "    word_obj[\"orig\"] = word\n",
    "    \n",
    "    try: word_obj[\"header\"] = re.search(\"^[^a-z]+\\w\", word).group()[:-1]\n",
    "    except: word_obj[\"header\"] = ''\n",
    "    try: word_obj[\"tail\"] = re.search(\"\\w[^a-z]+\", word).group()[1:]\n",
    "    except: word_obj[\"tail\"] = ''\n",
    "    try: word_obj[\"word\"] = re.search(\"\\w+\", word).group()\n",
    "    except: word_obj[\"word\"] = word\n",
    "        \n",
    "    word_obj[\"noun\"] = is_noun(word_obj[\"word\"])\n",
    "    word_obj[\"verb\"] = is_verb(word_obj[\"word\"])\n",
    "    word_obj[\"plural\"] = is_plural(word_obj[\"word\"])\n",
    "    \n",
    "    word_obj[\"tail\"] = \"quna\"*word_obj[\"plural\"] + word_obj[\"tail\"]\n",
    "    if word_obj[\"verb\"]: word_obj[\"object\"] = 1\n",
    "    word_obj = object_casing(word_obj)\n",
    "    word_obj = verb_conjugation(word_obj)\n",
    "    word_obj[\"context\"].append(word_obj[\"word\"])\n",
    "    if l(word_obj[\"context\"]) > 4:\n",
    "        word_obj[\"context\"] = word_obj[\"context\"][1:]\n",
    "    if \".\" in word_obj[\"tail\"]: \n",
    "        word_obj[\"object\"] = 0\n",
    "        word_obj[\"context\"] = []\n",
    "    return word_obj\n",
    "\n",
    "def postprocess(word_obj):\n",
    "    return word_obj[\"header\"] + word_obj[\"qic_word\"] + word_obj[\"tail\"]\n",
    "\n",
    "def title_case(text):\n",
    "    prev_word = \"\"\n",
    "    out = []\n",
    "    for word in text.split(\" \"):\n",
    "        if word == \"\": continue\n",
    "        if prev_word == \"\": word = word.title()\n",
    "        if \".\" in prev_word:    \n",
    "            word = word.title()\n",
    "        prev_word = word\n",
    "        out.append(word)\n",
    "    return \" \".join(out)\n",
    "\n",
    "def en2qic(raw, dictionary):\n",
    "    missing_words = []\n",
    "    truly_missing_words = []\n",
    "    translation = \"\"\n",
    "    used_words = []\n",
    "    word_obj = {\"context\": [], \"object\": 0}\n",
    "    for word in raw.lower().strip().split(' '):\n",
    "        word = word.strip()\n",
    "        if word == '': continue\n",
    "        word_obj = preprocess(word, word_obj)\n",
    "        word = word_obj[\"word\"]\n",
    "        try: qic_word = dictionary[word_obj[\"word\"]][0] + debug*f\" ({word},1.00)\"\n",
    "        except: \n",
    "            missing_words.append(word_obj[\"word\"])\n",
    "            if word_obj[\"word\"][-1] == 's':\n",
    "                word_obj[\"word\"] = word_obj[\"word\"][:-1]\n",
    "            try:\n",
    "                qic_word = dictionary[word_obj[\"word\"]][0] + debug*f\" ({word[:-1]},1.00)\"\n",
    "            except:\n",
    "                word_obj[\"word\"] += \"s\"\n",
    "                qic_word = find_similar(word_obj[\"word\"], dictionary)\n",
    "        word_obj[\"qic_word\"] = qic_word.split(\" \")[0]\n",
    "        if word_obj[\"qic_word\"] == word_obj[\"word\"]:\n",
    "            missing_words.append(word_obj[\"word\"])\n",
    "        if debug:\n",
    "            word_obj[\"tail\"] += \" \"+qic_word.split(\" \")[1]\n",
    "        qic_word = postprocess(word_obj)\n",
    "        translation += qic_word + \" \"\n",
    "        used_words.append([word, qic_word, word_obj[\"word\"], word_obj[\"tail\"]])\n",
    "    if debug:\n",
    "        for pair in used_words:\n",
    "            e = pair[0]\n",
    "            q = pair[1]\n",
    "            t = pair[3].split(\" \")[0]\n",
    "            print(e,' '*(15-l(e)),\"|\",q,t)\n",
    "        print(\"Words not in dictionary:\", missing_words)\n",
    "        print(\"Unable to match these words:\", truly_missing_words)\n",
    "    return translation\n",
    "\n",
    "dictionary = read_dict(\"dictionary_az\", dictionary)\n",
    "\n",
    "raw = input()\n",
    "if raw == \"\":\n",
    "    raw = default\n",
    "print(\"--------------------------------------------\")\n",
    "    \n",
    "en2qic(raw, dictionary)\n",
    "print(raw)\n",
    "debug = False\n",
    "print(title_case(en2qic(raw, dictionary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
